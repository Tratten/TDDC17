package tddc17;


import aima.core.environment.liuvacuum.*;
import aima.core.agent.Action;
import aima.core.agent.AgentProgram;
import aima.core.agent.Percept;
import aima.core.agent.impl.*;

import java.util.ArrayList;
import java.util.Collections;
import java.util.LinkedList;
import java.util.List;
import java.util.Queue;

import java.util.Random;

class MyAgentState
{
//	public int[][] world = new int[30][30];
	public Node[][] world = new Node[30][30];
	public int initialized = 0;
	final int UNKNOWN 	= 0;
	final int WALL 		= 1;
	final int CLEAR 	= 2;
	final int DIRT		= 3;
	final int HOME		= 4;
	final int EXPLORED	= 5;
	final int ACTION_NONE 			= 0;
	final int ACTION_MOVE_FORWARD 	= 1;
	final int ACTION_TURN_RIGHT 	= 2;
	final int ACTION_TURN_LEFT 		= 3;
	final int ACTION_SUCK	 		= 4;
	
	public boolean findHome = false;
	
	public int agent_x_position = 1;
	public int agent_y_position = 1;
	public int agent_last_action = ACTION_NONE;
	public int agent_next_action = ACTION_NONE;
	
	public static final int NORTH = 0;
	public static final int EAST = 1;
	public static final int SOUTH = 2;
	public static final int WEST = 3;
	public int agent_direction = EAST;

	public Queue<Integer> actionQueue = new LinkedList();
	
	
	MyAgentState()
	{
		for (int i=0; i < world.length; i++){
			for (int j=0; j < world[i].length ; j++){
				world[i][j] = new Node(UNKNOWN, i, j);
			}
		}
		world[1][1].type = HOME;
		agent_last_action = ACTION_NONE;
	}
	// Based on the last action and the received percept updates the x & y agent position
	public void updatePosition(DynamicPercept p)
	{
		Boolean bump = (Boolean)p.getAttribute("bump");

		if (agent_last_action==ACTION_MOVE_FORWARD && !bump)
	    {
			switch (agent_direction) {
			case MyAgentState.NORTH:
				agent_y_position--;
				break;
			case MyAgentState.EAST:
				agent_x_position++;
				break;
			case MyAgentState.SOUTH:
				agent_y_position++;
				break;
			case MyAgentState.WEST:
				agent_x_position--;
				break;
			}
	    }
		
	}
	
	public void updateWorld(int x_position, int y_position, int info)
	{
		if(world[x_position][y_position].type != HOME){
			world[x_position][y_position].type = info;
		}
		
	}
	
	public void printWorldDebug()
	{
		for (int i=0; i < world.length; i++)
		{
			for (int j=0; j < world[i].length ; j++)
			{
				if (world[j][i].type==UNKNOWN)
					System.out.print(" ? ");
				if (world[j][i].type==WALL)
					System.out.print(" # ");
				if (world[j][i].type==CLEAR)
					System.out.print(" C ");
				if (world[j][i].type==DIRT)
					System.out.print(" D ");
				if (world[j][i].type==HOME)
					System.out.print(" H ");

			}
			System.out.println("");
		}
	}
}

class MyAgentProgram implements AgentProgram {

	// Returns an array of coordinates array of tuples? to the closest of type target. Empty list otherwhise
	private Node bfs(int target){
		// Init
		for(int x = 0; x < state.world.length; x++){
			for(int y = 0; y < state.world[x].length; y++){
				state.world[x][y].parent = null;
				state.world[x][y].explored = false;
			}
		}
		Queue<Node> frontier = new LinkedList<Node>(); // queue containing coordinates
		Node currentNode = state.world[state.agent_x_position][state.agent_y_position];
		currentNode.explored = true;
		frontier.add(currentNode);
		
		while(!frontier.isEmpty()){
			currentNode = frontier.poll();
			
			List<Node> neighbours = findNeighbours(currentNode);
			
			for(int i = 0; i < neighbours.size(); i++){
				if(!neighbours.get(i).explored){
					neighbours.get(i).parent = currentNode;
					neighbours.get(i).explored = true;
					if(neighbours.get(i).type == target){
						//target found, return result
										
						return neighbours.get(i);
					}
					else {
						frontier.add(neighbours.get(i));
					}
				}
			}
		}

		// no solution found
		return null;
	}
	
	private List<Node> findNeighbours(Node origin){
		List<Node> result = new ArrayList<Node>();
		
		Node up = state.world[origin.x][origin.y-1];
		Node left = state.world[origin.x-1][origin.y];
		Node right = state.world[origin.x+1][origin.y];
		Node down = state.world[origin.x][origin.y+1];
		
		if(up.type != state.WALL){
			result.add(up);
		}
		
		if(left.type != state.WALL){
			result.add(left);
		}
		
		if(right.type != state.WALL){
			result.add(right);
		}
		
		if(down.type != state.WALL){
			result.add(down);
		}
		
		return result;
	}

	
	private Queue<Integer> goToGoal(List<Node> path, int dir, int x, int y){
		Queue<Integer> result = new LinkedList<Integer>();
		
		if(!path.isEmpty()) {
			for(int i = 0; i < path.size(); i++){
				int diffX = path.get(i).x - x;
				int diffY = path.get(i).y - y;
				
				if(diffX > 0){
					//east
					result = moveDir(result, dir, state.EAST);
					x++;
					dir = 1;
				} 
				else if(diffX < 0){
					//west
					result = moveDir(result, dir, state.WEST);
					x--;
					dir = 3;
				}
				else if(diffX == 0){
					if(diffY < 0){
						//north
						result = moveDir(result, dir, state.NORTH);
						y--;
						dir = 0;
					}
					else if(diffY > 0){
						//south
						result = moveDir(result, dir, state.SOUTH);
						y++;
						dir = 2;
					}
				}
			}
		}
		
		return result;
	}
	
	private int whichDir(int a, int b){
		int r = b-a % 4;
		if(Math.abs(r) == 3){
			r = (r*(-1))/3;
		}
		return r;
	}
	
	private Queue<Integer> moveDir(Queue<Integer> result, int myDir, int targetDir){
		System.out.println(myDir + " "+ targetDir + " " + whichDir(myDir, targetDir));
//		Queue<Integer> result = new LinkedList<Integer>();
		int dir = whichDir(myDir, targetDir);
		if(Math.abs(dir) == 3){
			System.out.println("HERE =======================================================");
		}
		while(dir > 0){
			System.out.println("hoger");
			result.add(state.ACTION_TURN_RIGHT);
			dir--;
		}
		while(dir < 0){
			System.out.println("venster");
			result.add(state.ACTION_TURN_LEFT);
			dir++;
		}
		
		result.add(state.ACTION_MOVE_FORWARD);
		return result;
	}
	
	private Action rotateRight() {
		state.agent_last_action = state.ACTION_TURN_RIGHT;
		state.agent_direction = ((state.agent_direction+1) % 4);
		return LIUVacuumEnvironment.ACTION_TURN_RIGHT;
	}
	
	private Action rotateLeft() {
		state.agent_last_action = state.ACTION_TURN_LEFT;
	    state.agent_direction = ((state.agent_direction-1) % 4);
	    if (state.agent_direction<0) 
	    	state.agent_direction +=4;
		return LIUVacuumEnvironment.ACTION_TURN_LEFT;
	}
	

	private int initnialRandomActions = 10;
	private Random random_generator = new Random();
	
	// Here you can define your variables!
	public int iterationCounter = 10000; // Changed this boundary! Was 10 before.
	public MyAgentState state = new MyAgentState();
	
	// moves the Agent to a random start position
	// uses percepts to update the Agent position - only the position, other percepts are ignored
	// returns a random action
	private Action moveToRandomStartPosition(DynamicPercept percept) {
		int action = random_generator.nextInt(6);
		initnialRandomActions--;
		state.updatePosition(percept);
		if(action==0) {
		    return rotateLeft();
		} else if (action==1) {
			return rotateRight();
		} 
		state.agent_last_action=state.ACTION_MOVE_FORWARD;
		return LIUVacuumEnvironment.ACTION_MOVE_FORWARD;
	}
	
	
	@Override
	public Action execute(Percept percept) {
		
		// DO NOT REMOVE this if condition!!!
    	if (initnialRandomActions>0) {
    		return moveToRandomStartPosition((DynamicPercept) percept);
    	} else if (initnialRandomActions==0) {
    		// process percept for the last step of the initial random actions
    		initnialRandomActions--;
    		state.updatePosition((DynamicPercept) percept);
			state.agent_last_action=state.ACTION_SUCK;
	    	return LIUVacuumEnvironment.ACTION_SUCK;
    	}
		
    	// This example agent program will update the internal agent state while only moving forward.
    	// START HERE - code below should be modified!
    	    	
	    iterationCounter--;
	    
	    if (iterationCounter==0) {
	    	return NoOpAction.NO_OP;
	    }
	    DynamicPercept p = (DynamicPercept) percept;
	    Boolean bump = (Boolean)p.getAttribute("bump");
	    Boolean dirt = (Boolean)p.getAttribute("dirt");
	    Boolean home = (Boolean)p.getAttribute("home");
	    System.out.println("percept: " + p);
	    
	    // State update based on the percept value and the last action
	    state.updatePosition((DynamicPercept)percept);
	    
	    System.out.println("x=" + state.agent_x_position);
    	System.out.println("y=" + state.agent_y_position);
    	System.out.println("dir=" + state.agent_direction);
    	
    	if (bump) {
    		System.out.println("Bump: " + state.actionQueue.peek());
			switch (state.agent_direction) {
			case MyAgentState.NORTH:
				state.updateWorld(state.agent_x_position,state.agent_y_position-1,state.WALL);
				break;
			case MyAgentState.EAST:
				state.updateWorld(state.agent_x_position+1,state.agent_y_position,state.WALL);
				break;
			case MyAgentState.SOUTH:
				state.updateWorld(state.agent_x_position,state.agent_y_position+1,state.WALL);
				break;
			case MyAgentState.WEST:
				state.updateWorld(state.agent_x_position-1,state.agent_y_position,state.WALL);
				break;
			}
	    }
	    if (dirt)
	    	state.updateWorld(state.agent_x_position,state.agent_y_position,state.DIRT);
	    else
	    	state.updateWorld(state.agent_x_position,state.agent_y_position,state.CLEAR);
	    
//	    state.printWorldDebug();
	    
	    // Next action selection based on the percept value
	    if (dirt)
	    {
	    	// if dirt -> Suck
	    	System.out.println("DIRT -> choosing SUCK action!");
	    	state.agent_last_action=state.ACTION_SUCK;
	    	return LIUVacuumEnvironment.ACTION_SUCK;
	    }
	    
	    if(state.findHome){
	    	if(state.agent_x_position == 1 && state.agent_y_position == 1){
	    		return NoOpAction.NO_OP;
	    	}
	    	else {
	    		Node homeBfs = bfs(state.HOME);
	    		if(homeBfs == null){
	    			return NoOpAction.NO_OP;
	    		}
	    		List<Node> homePath = followParents(homeBfs);
	    		state.actionQueue =  goToGoal(homePath, state.agent_direction, state.agent_x_position, state.agent_y_position);
	    		return performTask(state.actionQueue.poll(), p);
	    	}
	    }
	    else if(!state.actionQueue.isEmpty()){
	    	return performTask(state.actionQueue.poll(),p);
	    } else {
	    	Node bfs = bfs(state.UNKNOWN);
	    	if(bfs != null){
		    	List<Node> path = followParents(bfs);
		    	state.actionQueue =  goToGoal(path, state.agent_direction, state.agent_x_position, state.agent_y_position);
	    	}
		    else {
		    	state.findHome = true;
	    		Node homeBfs = bfs(state.HOME);
	    		if(homeBfs == null){
	    			return NoOpAction.NO_OP;
	    		}
	    		List<Node> homePath = followParents(homeBfs);
	    		state.actionQueue =  goToGoal(homePath, state.agent_direction, state.agent_x_position, state.agent_y_position);
	    	}
	    	return performTask(state.actionQueue.poll(), p);
	    }
	    
	}

	private List<Node> followParents(Node currentNode) {
		List<Node> result = new ArrayList<Node>();
		while(currentNode.parent != null){
			result.add(currentNode);
			currentNode = currentNode.parent;
		}
		
//		result.remove(result.size()-1);
		Collections.reverse(result);
		
		return result;
	}

	private Action performTask(int action, DynamicPercept p) {
		if(action == state.ACTION_MOVE_FORWARD){
			state.agent_last_action = state.ACTION_MOVE_FORWARD;
			return LIUVacuumEnvironment.ACTION_MOVE_FORWARD;
		}
		else if(action == state.ACTION_TURN_RIGHT){
			return rotateRight();
		}
		else if(action == state.ACTION_TURN_LEFT) {
			return rotateLeft();
		}
		else if(action == state.ACTION_NONE){
			return NoOpAction.NO_OP;
		}
		return null;
	}
	
}


public class MyVacuumAgent extends AbstractAgent {
    public MyVacuumAgent() {
    	super(new MyAgentProgram());
	}
}
